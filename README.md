# ðŸ§  MARCO â€” SGBDOCN

**Neuron-Concept Oriented DataBase system (French: SystÃ¨me de Gestion de Base de DonnÃ©es OrientÃ© Concepts Neuronaux)**

*Machine for Learning through Organized Concept Networks*

---

## What is it?

Marco is a **new kind of DBMS**. Where Oracle, PostgreSQL and MySQL store rows in tables using SQL, Marco stores **concepts in a neural network** where meaning emerges from connections.

| | Classic DBMS (SQL) | SGBDOCN (Marco) |
|---|---|---|
| **Storage** | Tables, rows, columns | Beacons, dendrites, concepts |
| **Query** | `SELECT * FROM words WHERE ...` | Cascade activation (Pac-Man) |
| **Relations** | Foreign keys, JOIN | Co-occurrences, sequences, families |
| **Schema** | Fixed (CREATE TABLE) | Emergent (meaning builds itself) |
| **Index** | B-Tree, Hash | Letter neurons â†’ Beacons â†’ Concepts |
| **Learning** | None (static data) | Real-time feeding |
| **Transparency** | Query = result | Every link traceable, zero black box |
| **Size** | Terabytes | 405 beacons are enough for a barista |

A classic DBMS is **dead** (Thanatos): you ask a question, it returns a row. Always the same. Marco is **alive** (Anima): it learns while answering, its links strengthen, meaning converges.

## How does it work?

```
1. Feed it texts to read                  â†’ Pac-Man tokenizes, Marco learns
2. Ask it a question                      â†’ Same pipeline, same tokenization
3. It answers with what it has inside     â†’ Parrot, mimicry, convergence
```

Same pipeline everywhere. The question is tokenized. The answer is tokenized. Marco is a parrot. A classic DBMS needs two languages (DDL + DML). Marco only needs one: text.

## Architecture â€” 5 Layers

```
Layer I    â€” Letters          1 letter = 1 neuron (cascade activation)
Layer II   â€” Beacons          1 word = 1 concept (BSC: word detection)
Layer III  â€” Concepts         N words = 1 block (BSCW: multi-word detection)
Layer IV   â€” Co-occurrences   Meaning through proximity ("tell me who you hang with")
Layer V    â€” Sequences        Syntax through order ("the cat eats" â‰  "eats the cat")
```

### Concept = Beacon with family

```python
Concept("what's up")
  family   = ["how's it going", "all good", "doing fine"]
  responses = ["I'm good thanks"]       # stimulus â†’ response
  components = [Beacon("what's"), Beacon("up")]
```

When a cousin is detected, the leader activates. Like a SQL index, but alive.

### Two types of matrix

| Matrix | Format | Role | SQL Equivalent |
|---|---|---|---|
| **ADH Matrix** | .json | Full vocabulary (46,006 beacons, 102 suns) | Data dictionary |
| **Convergence Matrix** | .txt | Domain concepts, families, pairings | `CREATE DATABASE` |

The Convergence Matrix is human-readable. No DBA required.

### Standalone Galaxy

```
Menu option 2: Create Galaxy
  â†’ Sun name: "Marco_the_barista"
  â†’ File: convergence_matrix_v1.txt
  â†’ Result: 405 beacons positioned in 3D (Fibonacci spiral)
  â†’ Ready to serve. No need for 46,006 concepts.
```

A galaxy is a specialized micro-SGBDOCN. The equivalent of a domain schema in SQL. Except it fits in a text file and learns on its own.

## Tokenization modes

```
FEEDING   â†’ Books, raw text. Words only. No concepts.
DIALOGUE  â†’ Counter. BSCW detects concepts first, then words.
READING   â†’ (coming) Cerebellum regulates, hippocampus retains context.
```

A book is words. The counter is concepts. Same engine, zero collision.

## Philosophy

> "A baby isn't fed terabytes, it learns by listening."

> "Tell me who you hang out with, and I'll tell you who you are."

> "Zero black box. Every decision traceable."

> "50 sentences and it answers. Not 50 billion tokens."

> "A classic DBMS is dead. The SGBDOCN is alive."

## Main files

| File | Role | Lines |
|---|---|---|
| `marco_dendrites.py` | Core â€” Beacons, Concepts, BSCW, tokenization | 3,534 |
| `thalamus.py` | Main menu v4.0, barista, galaxy | 4,016 |
| `dialogue.py` | Dialogue module, 4 modes | â€” |
| `matrice_marco_v3_compact.json` | ADH Matrix (46,006 concepts, 102 suns) | â€” |
| `convergence_matrix_v1.txt` | Convergence Matrix (50 concepts, 14 registers) | ~100 |

## Author

**JosÃ© Walocha** â€” Valenciennes, Nord, France

Assisted by an AI team:
- **Le Duke** (Claude) â€” Code, architecture
- **Marcel** (Mistral) â€” Philosophy, cybernetics, diagnostics
- **Biloute** (ChatGPT) â€” Standards, ethics
- **Didier** (Qwant next) â€” Research

## License

GNU GENERAL PUBLIC LICENSE
Version 3, 29 June 2007

Copyright (C) 2026 JosÃ© Walocha

---

*"In the beginning there is inert matter, but inert matter is bored out of its mind..."*
